{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eniola-Otukoya/Machine_Learning-Model/blob/main/Creating_my_own_chatbox.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checkpoint Objective\n",
        "In this exercise, you will learn how to create your own chatbot by modifying the code provided in the example in our course. You will need to choose a topic that you want your chatbot to be based on and find a text file related to that topic. Then, you will need to modify the code to preprocess the data in the text file and create a chatbot interface that can interact with the user.\n",
        "\n",
        "Instructions\n",
        "Choose a topic: Choose a topic that you are interested in and find a text file related to that topic. You can use websites such as Project Gutenberg to find free text files.\n",
        "Preprocess the data: Modify the preprocess() function in the code provided to preprocess the data in your text file. You may want to modify the stop words list or add additional preprocessing steps to better suit your needs.\n",
        "Define the similarity function: Modify the get_most_relevant_sentence() function to compute the similarity between the user's query and each sentence in your text file. You may want to modify the similarity metric or add additional features to improve the performance of your chatbot.\n",
        "Define the chatbot function: Modify the chatbot() function to return an appropriate response based on the most relevant sentence in your text file.\n",
        "Create a Streamlit app: Use the main() function in the code provided as a template to create a web-based chatbot interface. Prompt the user for a question, call the chatbot() function to get the response, and display it on the screen.\n",
        "Note:\n",
        "\n",
        "To run your code, you need to have the text file in the same directory as your Python script.\n",
        "\n",
        "You may want to test your chatbot with different types of questions to ensure that it is working correctly.\n",
        "\n",
        "You can continue to modify your chatbot to add additional features or improve its performance.\n"
      ],
      "metadata": {
        "id": "RDVvzE2pngin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yNYkXOASvIB",
        "outputId": "797fc34a-d15d-42d4-bfc4-1b5c1d368ff0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.29.0-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.2)\n",
            "Collecting importlib-metadata<7,>=1.4 (from streamlit)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.23.5)\n",
            "Requirement already satisfied: packaging<24,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (23.2)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.5.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=6.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.0.1)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.0)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.2.3)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.5.0)\n",
            "Requirement already satisfied: tzlocal<6,>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.2)\n",
            "Collecting validators<1,>=0.2 (from streamlit)\n",
            "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.2)\n",
            "Collecting watchdog>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7,>=1.4->streamlit) (3.17.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2023.11.17)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.15.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Installing collected packages: watchdog, validators, smmap, importlib-metadata, pydeck, gitdb, gitpython, streamlit\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 7.0.0\n",
            "    Uninstalling importlib-metadata-7.0.0:\n",
            "      Successfully uninstalled importlib-metadata-7.0.0\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.40 importlib-metadata-6.11.0 pydeck-0.8.1b0 smmap-5.0.1 streamlit-1.29.0 validators-0.22.0 watchdog-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import streamlit as st"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxBdygSXTXLE",
        "outputId": "d0d9f206-6f49-4f84-fb36-55c874b36354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The ‘nltk’ library is used for natural language processing tasks such as tokenization, lemmatization, and stopword removal. The ‘string’ library is used for string operations. The ‘streamlit’ library is used to create the web-based chatbot interface.\n",
        "The ‘nltk.download()’ function is used to download additional resources needed for the nltk library. In this case, we are downloading the punkt and averaged_perceptron_tagger resources. These resources are needed for tokenization and part-of-speech tagging tasks.\n",
        "Once you have imported the necessary libraries, you can use their functions and classes to perform various NLP tasks and create your chatbot"
      ],
      "metadata": {
        "id": "mO8h_XfaTcjm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and Preprocessing Data:\n",
        "The first step in building a chatbot is to load and preprocess the data that the chatbot will use to generate responses.\n",
        "In this example, we will load a text file and preprocess each sentence in the file to create a corpus that the chatbot can use to\n",
        "find the most relevant response."
      ],
      "metadata": {
        "id": "diIoKc20UXHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4DC9nJ1bax0",
        "outputId": "7dbe39a5-1b30-43e0-a80c-33766abb60d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "en2fK0y-d2xn",
        "outputId": "24f1e657-8f70-467f-e7b2-1476d97e8d76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the text file and preprocess the data\n",
        "with open('/content/drive/MyDrive/GOMYCODE/GOMYCODE CHECKPOINT 32 [creating my own chatbox]/health.txt', 'r', encoding='utf-8') as f:\n",
        "    data = f.read().replace('\\n', ' ')\n",
        "# Tokenize the text into sentences\n",
        "sentences = sent_tokenize(data)\n",
        "# Define a function to preprocess each sentence\n",
        "def preprocess(sentence):\n",
        "    # Tokenize the sentence into words\n",
        "    words = word_tokenize(sentence)\n",
        "    # Remove stopwords and punctuation\n",
        "    words = [word.lower() for word in words if word.lower() not in stopwords.words('english') and word not in string.punctuation]\n",
        "    # Lemmatize the words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return words\n",
        "\n",
        "# Preprocess each sentence in the text\n",
        "corpus = [preprocess(sentence) for sentence in sentences]\n",
        "\n"
      ],
      "metadata": {
        "id": "2OfDbI8tdREV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading and Preprocessing Data:\n",
        "First, we open the text file using the open() function and read the contents of the file using the read() method. We replace any newline characters (\\n) with a space character to ensure that each sentence is on a separate line.\n",
        "Next, we use the sent_tokenize() function from the nltk.tokenize module to tokenize the text into individual sentences.\n",
        "We then define a function called preprocess() that takes a sentence as input and performs the following preprocessing steps:\n",
        "1.\tTokenize the sentence into individual words using the word_tokenize() function from the nltk.tokenize module.\n",
        "2.\tRemove stopwords and punctuation from the list of words using a list comprehension. We use the stopwords.words('english') function from the nltk.corpus module to get a list of English stopwords, and the string.punctuation constant to get a string of all punctuation characters.\n",
        "3.\tLemmatize the words using the WordNetLemmatizer() class from the nltk.stem module. Lemmatization is the process of reducing a word to its base form (e.g., \"running\" to \"run\").\n",
        "Additional Resources"
      ],
      "metadata": {
        "id": "QlWM6Cqbg4NO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the Similarity Function:\n",
        "# Define a function to find the most relevant sentence given a query\n",
        "def get_most_relevant_sentence(query):\n",
        "    # Preprocess the query\n",
        "    query = preprocess(query)\n",
        "    # Compute the similarity between the query and each sentence in the text\n",
        "    max_similarity = 0\n",
        "    most_relevant_sentence = \"\"\n",
        "    for sentence in corpus:\n",
        "        similarity = len(set(query).intersection(sentence)) / float(len(set(query).union(sentence)))\n",
        "        if similarity > max_similarity:\n",
        "            max_similarity = similarity\n",
        "            most_relevant_sentence = \" \".join(sentence)\n",
        "    return most_relevant_sentence\n",
        "\n"
      ],
      "metadata": {
        "id": "I4CwREw1hMmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The get_most_relevant_sentence() function is responsible for finding the most relevant sentence in the corpus given a user query.\n",
        " Here's how it works:\n",
        "\n",
        "\n",
        "Defining the Similarity Function:\n",
        "1.\tPreprocess the user query using the preprocess() function defined earlier.\n",
        "2.\tIterate over each sentence in the corpus.\n",
        "3.\tCompute the similarity between the preprocessed query and the current sentence using the Jaccard similarity coefficient. The Jaccard similarity coefficient is a measure of similarity between two sets and is defined as the size of the intersection divided by the size of the union of the sets. In this case, we treat the preprocessed query and each sentence in the corpus as sets of words and compute their Jaccard similarity coefficient.\n",
        "4.\tUpdate the most relevant sentence if the current sentence has a higher similarity score.\n",
        "5.\tReturn the most relevant sentence.\n"
      ],
      "metadata": {
        "id": "xKV7B6QLharh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The chatbot Function:\n",
        "def chatbot(question):\n",
        "    # Find the most relevant sentence\n",
        "    most_relevant_sentence = get_most_relevant_sentence(question)\n",
        "    # Return the answer\n",
        "    return most_relevant_sentence"
      ],
      "metadata": {
        "id": "8JAsHonQhpfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chatbot() function is the main function that takes a user's question as input, processes it using the get_most_relevant_sentence() function, and returns the most relevant sentence as the chatbot's response.\n",
        "Here's how it works:\n",
        "\n",
        "The chatbot Function:\n",
        "1.\tThe chatbot() function takes a user's question as input.\n",
        "2.\tIt calls the get_most_relevant_sentence() function to get the most relevant sentence from the corpus that matches the user's query.\n",
        "3.\tIt returns the most relevant sentence as the chatbot's response."
      ],
      "metadata": {
        "id": "WWlFYRCJhzKt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n74-Vu5OnX0-",
        "outputId": "63131115-0dc7-439f-c973-89f92d9d7abf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-12-21 08:44:45.000 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n"
          ]
        }
      ],
      "source": [
        "# Creating a Streamlit App :\n",
        "# The main() function creates a Streamlit app that provides a user interface for the chatbot. Here's how it works:\n",
        "# Create a Streamlit app\n",
        "\n",
        "def main():\n",
        "    st.title(\"Chatbot\")\n",
        "    st.write(\"Hello! I'm a chatbot designed by Clifford. Ask me any health related question\")\n",
        "    # Get the user's question\n",
        "    question = st.text_input(\"User:\")\n",
        "    # Create a button to submit the question\n",
        "    if st.button(\"Submit\"):\n",
        "        # Call the chatbot function with the question and display the response\n",
        "        response = chatbot(question)\n",
        "        st.write(\"Chatbot: \" + response)\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion :\n",
        "In summary, the code provided defines a simple chatbot using Python's Natural Language Toolkit (NLTK) and Streamlit. The chatbot is designed to provide answers to questions related to a specific topic, as described in a text file.\n",
        "The code consists of several functions:\n",
        "\n",
        "•\tpreprocess(): This function preprocesses a sentence by tokenizing it into words, removing stopwords and punctuation, and lemmatizing the words.\n",
        "\n",
        "•\tget_most_relevant_sentence(): This function finds the most relevant sentence in the text file given a user query. It does this by computing the similarity between the query and each sentence in the text file and returning the sentence with the highest similarity score.\n",
        "\n",
        "•\tchatbot(): This function uses the get_most_relevant_sentence() function to get the most relevant sentence for a given user question and returns it as the chatbot's response.\n",
        "\n",
        "•\tmain(): This function creates a Streamlit app that provides a user interface for the chatbot. It prompts the user to enter a question and displays the chatbot's response on the screen.\n",
        "\n",
        "Overall, the chatbot is a simple example of how NLTK and Streamlit can be used to create a conversational interface for answering questions related to a specific topic. With further development and refinement, the chatbot could be made more robust and capable of answering a wider range of questions.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hBzNDFW5iUFs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GlEtLfhvijq9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}